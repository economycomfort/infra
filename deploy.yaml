---
## Handles initial deployment and configuration of defined hosts.

  - hosts: all
    become: yes
    vars_files:
      - 'vars/vault.yaml'
    
    pre_tasks:     
      - name: import keys from github
        ansible.posix.authorized_key:
          user: "{{ ansible_user }}"
          state: present
          key: https://github.com/economycomfort.keys

    roles:
      - role: grog.package
      - role: dab-motd
      - { role: dab-dotfiles, become: no }

  #################
  ###  proxmox  ###
  #################
  - hosts: proxmox:tinyprox
    tasks:
      # Passes through primary user to be able to log in to web interface.
      # Needs a user created (probably david) and passwordless sudo.
      - name: Add proxmox web interface users
        template:
          src: proxmox/user.cfg.j2  # relative to main playbook dir
          dest: /etc/pve/user.cfg
      
      # Disable clustering services
      - name: Disable clustering services
        command: "systemctl disable --now {{ item }}"
        loop:
          - pve-ha-crm.service
          - pve-ha-lrm.service
          - corosync.service
          #- pvesr.timer # possibly not applicable in Proxmox 7.x+?

      # Copies shell script which eases cloud-init template creation    
      - name: Copy cloud-init template deployment script
        template:
          src: proxmox/deploy-cloud-init-template.sh.j2
          dest: /root/deploy-cloud-init-template.sh
          mode: 0744

      # Copies proxmox config backup script
      - name: Copy proxmox configuration backup script
        template:
          src: proxmox/pvehost-backup.j2
          dest: /etc/cron.daily/pvehost-backup
          mode: 0755

      # Add required IOMMU kernel modules
      - name: Add required IOMMU kernel modules
        template:
          src: proxmox/iommu.j2
          dest: /etc/modules-load.d/iommu.conf
          mode: 0644

      # Modify GRUB config to enable IOMMU (hardware passthrough)
      - name: Enable IOMMU in GRUB config
        lineinfile:
          path: /etc/default/grub
          regexp: '^GRUB_CMDLINE_LINUX_DEFAULT='
          line: GRUB_CMDLINE_LINUX_DEFAULT="quiet intel_iommu=on iommu=pt"

      # Update GRUB
      - name: Update GRUB
        command: update-grub

      # Warn about reboot required
      - name: REBOOT REQUIRED
        ansible.builtin.debug:
          msg: "NOTICE: REBOOT REQUIRED TO ENABLE HARDWARE PASSTHROUGH"

    roles:
      - role: simoncaron.pve_addons
      - role: geerlingguy.ntp
 
  ###############
  ###  servo  ###
  ###############
  - hosts: servo
    become: yes
    vars_files:
      - 'vars/vault.yaml'
    
    pre_tasks:
      # Checks to see if defined ZFS pools are mounted. Playbook fails if not.
      # These pools should be mounted before running this playbook.
      - name: Ensure ZFS pool(s) are imported and mounted before proceeding
        community.general.zpool_facts:
          pool: "{{ item.name }}"
        loop: "{{ zpools }}"
      
      # Ensures that defined ZFS datasets are mounted.
      - name: Gathering facts about ZFS datasets
        community.general.zfs_facts:
          dataset: "{{ item.name }}"
          recurse: false
        loop: "{{ zdatasets }}"
        register: dab_datasets

      - set_fact:
          mount_status: "{{ dab_datasets.results | map(attribute='ansible_facts') | flatten | map(attribute='ansible_zfs_datasets') | flatten | map(attribute='mounted') }}"
          mount_name: "{{ zdatasets | map(attribute='name') }}"

      - name: Fail if a ZFS dataset is not mounted.
        ansible.builtin.fail:
          msg: "{{ mount_name[ansible_loop.index0] }} is not mounted!"
        loop: "{{ mount_name }}"
        loop_control: 
          extended: true
        when: mount_status[ansible_loop.index0] == 'no'

      # On the storage host, we're using an alternate path for /home.
      # The following tasks create a symlink for continuity.
      - name: Checking for existence of zhome directory
        ansible.builtin.stat:
          path: "{{ zhome }}"
        register: zfshome
      
      - name: Discovering facts about /home
        ansible.builtin.stat:
          path: /home
        register: oldhome
      
      # These commands must be combined; if not, our users $HOME will
      # be absent and subsequent SSH connections in the playbook will fail.
      - name: Symlink /home to zhome
        ansible.builtin.shell: 
          cmd: "mv /home /home.orig && ln -sf {{ zhome }} /home"
        when: 
          - zfshome.stat.exists == True
          - oldhome.stat.islnk == False
    
    roles:
      - role: buluma.mount
      - role: geerlingguy.docker
      #- role: geerlingguy.nfs
      - role: vladgh.samba.server

    tasks:
      # Sanoid configuration for automated ZFS snapshots
      - name: Create /etc/sanoid directory
        ansible.builtin.file:
          path: /etc/sanoid
          state: directory
          mode: 0755

      - name: Copy sanoid configuration template
        ansible.builtin.template:
          src: sanoid.conf.j2
          dest: /etc/sanoid/sanoid.conf
          owner: root
          group: root
          mode: 0644

  #################
  ###  homebot  ###
  #################
  - hosts: homebot
    become: yes
    vars_files:
      - 'vars/vault.yaml'
    roles:
      - role: geerlingguy.docker

  ######################
  ###  docker-media  ###
  ######################
  - hosts: docker-media
    become: yes
    vars_files:
      - 'vars/vault.yaml'

    roles:
      - role: buluma.mount
      - role: geerlingguy.docker
  
  ###################
  ###  pihole(s)  ###
  ###################
  - hosts: pihole
    become: yes
    vars_files:
      - 'vars/vault.yaml'
    tasks:
      - name: "Install Pihole"
        ansible.builtin.command: "curl -sSL https://install.pi-hole.net | bash"

      #- name: 
  
  #################
  ###  sandbox  ###
  #################
  - hosts: sandbox
    become: yes
    roles:

  ######################
  ###  Vectra Hosts  ###
  ######################
  - hosts: vectra
    become: yes
    tasks:
      - name: Ensure correct hostname
        ansible.builtin.hostname:
          name: "{{ inventory_hostname }}"
